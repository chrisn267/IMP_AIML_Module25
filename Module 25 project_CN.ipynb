{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 25 Project Prompt\n",
    "\n",
    "In this project, you'll participate in [Kaggle's Digit Recognizer](https://www.kaggle.com/c/digit-recognizer) competition using a neural network.\n",
    "\n",
    "From Kaggle:\n",
    "\n",
    "> MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    "> In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use your preferred Neural Network framework. If using PyTorch, the example in the [documentation](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models) will help you get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "\n",
    "Load `train.csv` from Kaggle into a pandas DataFrame or use [PyTorch dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#datasets-dataloaders) to load in the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"data\",train=True,download=True,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "We want to create a validation set that the model will never see to approximate how it's going to do with Kaggle's `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = random_split(train_dataset, [50000,10000],generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=1000, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your neural network\n",
    "\n",
    "Using your preferred framework (PyTorch or Keras), set up your network. The documentation from [Pytorch](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) may be helpful to get started.\n",
    "\n",
    "In PyTorch, you will define the model class here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out, drop_rate):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, D_out)\n",
    "        self.dropout = torch.nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h1_relu = nn.functional.relu(self.linear1(x))\n",
    "        h2_relu = nn.functional.relu(self.linear2(h1_relu))\n",
    "        dropout = self.dropout(h1_relu+h2_relu)  #to combine relu ouput both must be same size\n",
    "        y_pred = self.linear3(dropout)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile your model (if using `Keras`) and define optimizers\n",
    "\n",
    "Since this is a multiclass classification problem, your loss function is `categorical_crossentropy` for Keras or `CrossEntropyLoss()` for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_in is input dimension;\n",
    "# H1 is hidden dimension; \n",
    "# H2 is hidden dimension;\n",
    "# D_out is output dimension.\n",
    "\n",
    "D_in = 28*28\n",
    "H1 = 64\n",
    "H2 = 64\n",
    "D_out = 10\n",
    "drop_rate = 0.5\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = ThreeLayerNet(D_in, H1, H2, D_out, drop_rate)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "learn_rate = 0.01\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "- Keras: Use your X_test, y_test from the `train_test_split` step for the `validation_data` parameter.\n",
    "- PyTorch: Train the model by running a loop of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 2.31\n",
      "Epoch 1, train loss: 2.21\n",
      "Epoch 1, train loss: 2.09\n",
      "Epoch 1, train loss: 1.96\n",
      "Epoch 1, train loss: 1.82\n",
      "Epoch 1, validation loss: 0.97\n",
      "Epoch 2, train loss: 0.96\n",
      "Epoch 2, train loss: 0.86\n",
      "Epoch 2, train loss: 0.79\n",
      "Epoch 2, train loss: 0.73\n",
      "Epoch 2, train loss: 0.69\n",
      "Epoch 2, validation loss: 0.53\n",
      "Epoch 3, train loss: 0.40\n",
      "Epoch 3, train loss: 0.48\n",
      "Epoch 3, train loss: 0.47\n",
      "Epoch 3, train loss: 0.46\n",
      "Epoch 3, train loss: 0.45\n",
      "Epoch 3, validation loss: 0.44\n",
      "Epoch 4, train loss: 0.36\n",
      "Epoch 4, train loss: 0.40\n",
      "Epoch 4, train loss: 0.39\n",
      "Epoch 4, train loss: 0.39\n",
      "Epoch 4, train loss: 0.39\n",
      "Epoch 4, validation loss: 0.40\n",
      "Epoch 5, train loss: 0.28\n",
      "Epoch 5, train loss: 0.35\n",
      "Epoch 5, train loss: 0.35\n",
      "Epoch 5, train loss: 0.35\n",
      "Epoch 5, train loss: 0.35\n",
      "Epoch 5, validation loss: 0.38\n",
      "Epoch 6, train loss: 0.33\n",
      "Epoch 6, train loss: 0.33\n",
      "Epoch 6, train loss: 0.33\n",
      "Epoch 6, train loss: 0.33\n",
      "Epoch 6, train loss: 0.33\n",
      "Epoch 6, validation loss: 0.37\n",
      "Epoch 7, train loss: 0.27\n",
      "Epoch 7, train loss: 0.31\n",
      "Epoch 7, train loss: 0.31\n",
      "Epoch 7, train loss: 0.31\n",
      "Epoch 7, train loss: 0.32\n",
      "Epoch 7, validation loss: 0.35\n",
      "Epoch 8, train loss: 0.20\n",
      "Epoch 8, train loss: 0.30\n",
      "Epoch 8, train loss: 0.29\n",
      "Epoch 8, train loss: 0.30\n",
      "Epoch 8, train loss: 0.30\n",
      "Epoch 8, validation loss: 0.34\n",
      "Epoch 9, train loss: 0.22\n",
      "Epoch 9, train loss: 0.28\n",
      "Epoch 9, train loss: 0.29\n",
      "Epoch 9, train loss: 0.29\n",
      "Epoch 9, train loss: 0.28\n",
      "Epoch 9, validation loss: 0.33\n",
      "Epoch 10, train loss: 0.29\n",
      "Epoch 10, train loss: 0.26\n",
      "Epoch 10, train loss: 0.27\n",
      "Epoch 10, train loss: 0.27\n",
      "Epoch 10, train loss: 0.27\n",
      "Epoch 10, validation loss: 0.32\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    \n",
    "    track_losses_t = list()\n",
    "    for ii, batch in enumerate(train_loader):\n",
    "        X, y = batch\n",
    "        \n",
    "        # reshape\n",
    "        b = X.size(0)\n",
    "        X = X.view(b, -1)\n",
    "        \n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform a backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        track_losses_t.append(loss.item())\n",
    "        \n",
    "        # print loss\n",
    "        if ii%100==0:\n",
    "            print(f\"Epoch {ep + 1}, train loss: {torch.tensor(track_losses_t).mean():.2f}\")\n",
    "        #print(y[0])\n",
    "        #print(y_pred[0])\n",
    "        \n",
    "    track_losses_v = list()\n",
    "    model.eval()\n",
    "    for ii, batch in enumerate(valid_loader):\n",
    "        X, y = batch\n",
    "        b = X.size(0)\n",
    "        X = X.view(b, -1)\n",
    "        \n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        track_losses_v.append(loss.item())\n",
    "        \n",
    "        # print loss\n",
    "        if ii%100==0:\n",
    "            print(f\"Epoch {ep + 1}, validation loss: {torch.tensor(track_losses_v).mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Kaggle's `test.csv`\n",
    "\n",
    "Be sure to do the **same** preprocessing you did for your training `X`.\n",
    "\n",
    "You can use `dataloader` if using PyTorch or pandas as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root=\"data\",train=False,download=True,transform=ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=10000, shuffle = False)\n",
    "test_dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your predictions and Overall performance (loss)\n",
    "\n",
    "Display a few predictions and display a confusion matrix. Explain where your model fails with 1-2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 960    0    2    1    0    5    9    1    2    0]\n",
      " [   0 1114    2    2    0    2    4    2    9    0]\n",
      " [   9    5  933   15   11    1   14   12   26    6]\n",
      " [   2    1   21  928    0   21    2   14   14    7]\n",
      " [   1    2    4    1  917    0   14    2    4   37]\n",
      " [  10    3    5   47    5  764   18    6   27    7]\n",
      " [  11    3    7    1    9   11  912    2    2    0]\n",
      " [   2    9   29    7    5    1    0  946    2   27]\n",
      " [   9   10    7   29    9   22   13    9  858    8]\n",
      " [  12   10    2   12   36    6    1   16    7  907]]\n",
      "accuracy:  0.9239\n"
     ]
    }
   ],
   "source": [
    "for ii, batch in enumerate(test_loader):\n",
    "    \n",
    "    X, y = batch\n",
    "    \n",
    "    # reshape\n",
    "    b = X.size(0)\n",
    "    X = X.view(b, -1)\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "\n",
    "orig_y = y.numpy()\n",
    "pred_y = np.argmax(y_pred.numpy(), axis = 1)\n",
    "\n",
    "print(confusion_matrix(orig_y, pred_y))\n",
    "print(\"accuracy: \", sum(orig_y == pred_y)/len(orig_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a hyperparameter and attempt to use your Bayesian Optimization code to optimize\n",
    "\n",
    "After retraining the model with the \"winning\" hyperparameter value, tell us if it improved your loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condense the work above into a single function which takes four hyperparamters and returns the model accuracy\n",
    "\n",
    "def train_and_run_model(learning_rate, drop_rate, hl1_size, hl2_size):\n",
    "\n",
    "    D_in = 28*28\n",
    "    H1 = hl1_size\n",
    "    H2 = hl2_size\n",
    "    D_out = 10\n",
    "    drop_rate = drop_rate\n",
    "\n",
    "    # Construct our model by instantiating the class defined above\n",
    "    model = ThreeLayerNet(D_in, H1, H2, D_out, drop_rate)\n",
    "\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "    learn_rate = learning_rate\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learn_rate)\n",
    "    \n",
    "    n_epoch = 5\n",
    "\n",
    "    for ep in range(n_epoch):\n",
    "\n",
    "        for ii, batch in enumerate(train_loader):\n",
    "            X, y = batch\n",
    "            b = X.size(0)\n",
    "            X = X.view(b, -1)\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    for ii, batch in enumerate(valid_loader):\n",
    "        if ii == 0:\n",
    "            X, y = batch\n",
    "            b = X.size(0)\n",
    "            X = X.view(b, -1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X)\n",
    "\n",
    "            orig_y = y.numpy()\n",
    "            pred_y = np.argmax(y_pred.numpy(), axis = 1)\n",
    "\n",
    "            accuracy = sum(orig_y == pred_y)/len(orig_y)\n",
    "    \n",
    "    return(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalise grid for plots\n",
    "# learning rate (0.00001 to 0.5)\n",
    "# dropout rate (0 to 0.6)\n",
    "# hl1_size (40 to 200)\n",
    "# hl2_size (pair with h1)\n",
    "\n",
    "lr_coords = np.array([0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5])\n",
    "do_coords = np.linspace(0, 0.6, 13)\n",
    "h1_coords = np.array([40, 60, 80, 100, 120, 140, 160, 180, 200])\n",
    "h2_coords = 1\n",
    "\n",
    "lr_vals, do_vals, h1_vals, h2_vals = np.meshgrid(lr_coords, do_coords, h1_coords, h2_coords)\n",
    "lr_vals = lr_vals.reshape(-1,1)\n",
    "do_vals = do_vals.reshape(-1,1)\n",
    "h1_vals = h1_vals.reshape(-1,1)\n",
    "h2_vals = h2_vals.reshape(-1,1)\n",
    "x_vals = np.column_stack((lr_vals,do_vals,h1_vals,h2_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  0\n",
      "run  1\n",
      "run  2\n",
      "run  3\n",
      "run  4\n",
      "run  5\n",
      "run  6\n",
      "run  7\n",
      "run  8\n",
      "run  9\n"
     ]
    }
   ],
   "source": [
    "# run our function 10 times to get some intial values\n",
    "\n",
    "init_runs = 10\n",
    "lr_init = np.random.choice(lr_coords, size = init_runs)\n",
    "do_init = np.random.choice(do_coords, size = init_runs)\n",
    "h1_init = np.random.choice(h1_coords, size = init_runs)\n",
    "h2_init = np.random.choice(h2_coords, size = init_runs)\n",
    "inputX = np.zeros((10, 4))\n",
    "outputY = np.zeros(10)\n",
    "\n",
    "for ii in range(init_runs): \n",
    "    inputX[ii,0] = lr_init[ii]\n",
    "    inputX[ii,1] = do_init[ii]\n",
    "    inputX[ii,2] = h1_init[ii]\n",
    "    inputX[ii,3] = h2_init[ii]\n",
    "    # run model - use h2 = h1\n",
    "    outputY[ii] = train_and_run_model(lr_init[ii], do_init[ii], h1_init[ii], h1_init[ii])\n",
    "    print(\"run \",ii)\n",
    "    \n",
    "max_Y = max(outputY)\n",
    "lr_opt = lr_init[np.argmax(outputY)]\n",
    "do_opt = do_init[np.argmax(outputY)]\n",
    "h1_opt = h1_init[np.argmax(outputY)]\n",
    "h2_opt = h2_init[np.argmax(outputY)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.145, 0.969, 0.939, 0.879, 0.731, 0.966, 0.096, 0.972, 0.76 ,\n",
       "       0.733])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now bring in our bayesian optimisation function from the capstone\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "import scipy.stats as sps\n",
    "\n",
    "def run_bayesian(X, Y, aqfn = \"UCB\", eta = 0.05):\n",
    "\n",
    "    # set up and run model\n",
    "    constant_value = 1.0            # 1.0             #\n",
    "    constant_value_bounds=\"fixed\"   # 'fixed'         # parameters optimised during fit if not 'fixed'\n",
    "    rbf_lengthscale = 0.1           # 1.0             # small gives more variability in x - so less exploring \n",
    "    length_scale_bounds=\"fixed\"     # 'fixed'         # parameters optimised during fit if not 'fixed'\n",
    "\n",
    "    kernel1 = ConstantKernel(constant_value, constant_value_bounds) * RBF(rbf_lengthscale, length_scale_bounds)\n",
    "    alpha1 = 1e-10                  # 1e-10           # high noise igonres data, low noise overfits\n",
    "    optimizer1 = 'fmin_l_bfgs_b'    # 'fmin_l_bfgs_b' # choose how hyperparameters are optimised\n",
    "    n_restarts_optimizer1 = 0       # 0               # how many times is the hyperparameter optimistion run?\n",
    "    normalize_y1 = True             # False           # are Y (target) value normalised?\n",
    "    copy_X_train1 = True            # True            # store trianing data in object\n",
    "    random_state1 = None            # None            # set a random seed\n",
    "\n",
    "    beta = 1.96                     # 1.96            # standrd deviation used in acquisition function\n",
    "\n",
    "    model2 = GaussianProcessRegressor(kernel = kernel1, \n",
    "                                     alpha = alpha1, \n",
    "                                     optimizer = optimizer1, \n",
    "                                     n_restarts_optimizer = n_restarts_optimizer1, \n",
    "                                     normalize_y = normalize_y1,\n",
    "                                     copy_X_train = copy_X_train1,\n",
    "                                     random_state = random_state1) \n",
    "\n",
    "    # fit model\n",
    "    model2.fit(X, Y)\n",
    "\n",
    "    # calculate mean and standard devation, make them one-dimensional for plotting\n",
    "    posterior_mean, posterior_std = model2.predict(x_vals, return_std = True)\n",
    "    posterior_mean, posterior_std = posterior_mean.squeeze(), posterior_std.squeeze()\n",
    "\n",
    "    # MODIFY THE CODE IN THIS AREA\n",
    "    # COMMENT OUT ALL BUT ONE FUNCTION\n",
    "    #######################################################\n",
    "\n",
    "    if aqfn == \"max_var\":\n",
    "        # Max Variance  \n",
    "        acquisition_function = posterior_std ** 2    \n",
    "    \n",
    "    elif aqfn == \"POI\":\n",
    "        # Prob of Improvement. eta = 0.05\n",
    "        #eta = 0.05\n",
    "        acquisition_function = sps.norm.cdf((posterior_mean - (max_Y + eta)) / posterior_std)    \n",
    "    \n",
    "    else:  #UCB by default\n",
    "        # Upper Confidence Bound\n",
    "        acquisition_function = posterior_mean + beta * posterior_std\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    lr_next = lr_vals[np.argmax(acquisition_function)]\n",
    "    do_next = do_vals[np.argmax(acquisition_function)]\n",
    "    h1_next = h1_vals[np.argmax(acquisition_function)]\n",
    "    h2_next = h2_vals[np.argmax(acquisition_function)]\n",
    "\n",
    "\n",
    "    #print(\"next:\", lr_next, do_next, h1_next, h2_next)\n",
    "    return lr_next[0], do_next[0], h1_next[0], h2_next[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now write a main loop which uses the initial 10 runs to pre-load the optimisation, \n",
    "# then runs 100 loops to find an optimal value\n",
    "\n",
    "## WARNING THIS TAKE 30 minutes per loop to run, so 1.5 hours in total ##\n",
    "\n",
    "def optimising_loop(X, Y, runs = 100, aqfn = \"UCB\", eta = 0.05):\n",
    "    newX = X\n",
    "    newY = Y\n",
    "    tic = time.perf_counter()\n",
    "    for ii in range(runs):\n",
    "        lr, do, h1, h2 = run_bayesian(newX, newY, aqfn, eta)\n",
    "        print(lr,do,h1)\n",
    "        accuracy = train_and_run_model(lr, do, h1, h1)\n",
    "        newX = np.vstack((newX, np.array([lr, do, h1, h2])))\n",
    "        newY = np.hstack((newY, accuracy))\n",
    "        bestY = newY[np.argmax(newY)]\n",
    "        bestX = newX[np.argmax(newY)]\n",
    "        print(bestY)\n",
    "    toc = time.perf_counter()\n",
    "    print(round(toc - tic,4))\n",
    "    return newX, newY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0.0 40\n",
      "0.972\n",
      "1e-05 0.0 60\n",
      "0.972\n",
      "1e-05 0.0 80\n",
      "0.972\n",
      "1e-05 0.0 100\n",
      "0.972\n",
      "1e-05 0.0 120\n",
      "0.972\n",
      "1e-05 0.0 140\n",
      "0.972\n",
      "1e-05 0.0 160\n",
      "0.972\n",
      "1e-05 0.0 180\n",
      "0.972\n",
      "1e-05 0.0 200\n",
      "0.972\n",
      "0.5 0.6 60\n",
      "0.972\n",
      "0.5 0.49999999999999994 60\n",
      "0.972\n",
      "0.5 0.35 60\n",
      "0.972\n",
      "0.5 0.19999999999999998 60\n",
      "0.972\n",
      "0.5 0.049999999999999996 60\n",
      "0.972\n",
      "0.1 0.5499999999999999 60\n",
      "0.972\n",
      "0.01 0.44999999999999996 60\n",
      "0.972\n",
      "1e-05 0.6 60\n",
      "0.972\n",
      "0.1 0.39999999999999997 60\n",
      "0.972\n",
      "1e-05 0.3 60\n",
      "0.972\n",
      "0.1 0.44999999999999996 60\n",
      "0.972\n",
      "0.5 0.6 160\n",
      "0.972\n",
      "0.5 0.44999999999999996 160\n",
      "0.972\n",
      "0.5 0.3 160\n",
      "0.972\n",
      "0.5 0.15 160\n",
      "0.972\n",
      "0.5 0.0 160\n",
      "0.974\n",
      "0.1 0.5499999999999999 160\n",
      "0.974\n",
      "0.005 0.44999999999999996 160\n",
      "0.974\n",
      "1e-05 0.6 160\n",
      "0.974\n",
      "0.1 0.39999999999999997 160\n",
      "0.974\n",
      "1e-05 0.3 160\n",
      "0.974\n",
      "0.5 0.6 80\n",
      "0.974\n",
      "0.5 0.44999999999999996 80\n",
      "0.974\n",
      "0.5 0.3 80\n",
      "0.974\n",
      "0.5 0.15 80\n",
      "0.974\n",
      "0.5 0.0 80\n",
      "0.974\n",
      "0.1 0.5499999999999999 80\n",
      "0.974\n",
      "1e-05 0.44999999999999996 80\n",
      "0.974\n",
      "0.5 0.6 120\n",
      "0.974\n",
      "0.5 0.44999999999999996 120\n",
      "0.974\n",
      "0.5 0.3 120\n",
      "0.976\n",
      "0.5 0.15 120\n",
      "0.976\n",
      "0.5 0.0 120\n",
      "0.976\n",
      "0.1 0.5499999999999999 120\n",
      "0.976\n",
      "0.1 0.39999999999999997 120\n",
      "0.976\n",
      "1e-05 0.49999999999999994 120\n",
      "0.976\n",
      "0.1 0.24999999999999997 120\n",
      "0.976\n",
      "0.5 0.6 40\n",
      "0.976\n",
      "0.5 0.44999999999999996 40\n",
      "0.976\n",
      "0.5 0.3 40\n",
      "0.976\n",
      "0.5 0.15 40\n",
      "0.976\n",
      "0.5 0.0 40\n",
      "0.976\n",
      "0.1 0.5499999999999999 40\n",
      "0.976\n",
      "0.05 0.39999999999999997 40\n",
      "0.976\n",
      "1e-05 0.6 40\n",
      "0.976\n",
      "0.5 0.6 200\n",
      "0.976\n",
      "0.5 0.44999999999999996 200\n",
      "0.976\n",
      "0.5 0.3 200\n",
      "0.976\n",
      "0.5 0.15 200\n",
      "0.976\n",
      "0.5 0.0 200\n",
      "0.976\n",
      "0.1 0.5499999999999999 200\n",
      "0.976\n",
      "0.1 0.39999999999999997 200\n",
      "0.976\n",
      "1e-05 0.44999999999999996 200\n",
      "0.976\n",
      "0.1 0.24999999999999997 200\n",
      "0.976\n",
      "0.5 0.6 140\n",
      "0.976\n",
      "0.5 0.44999999999999996 140\n",
      "0.976\n",
      "0.5 0.24999999999999997 140\n",
      "0.976\n",
      "0.5 0.09999999999999999 140\n",
      "0.977\n",
      "0.1 0.5499999999999999 140\n",
      "0.977\n",
      "0.05 0.39999999999999997 140\n",
      "0.977\n",
      "1e-05 0.6 140\n",
      "0.977\n",
      "0.5 0.6 180\n",
      "0.977\n",
      "0.5 0.44999999999999996 180\n",
      "0.977\n",
      "0.5 0.24999999999999997 180\n",
      "0.977\n",
      "0.5 0.09999999999999999 180\n",
      "0.977\n",
      "0.1 0.5499999999999999 180\n",
      "0.977\n",
      "0.05 0.39999999999999997 180\n",
      "0.977\n",
      "0.5 0.6 100\n",
      "0.977\n",
      "0.5 0.44999999999999996 100\n",
      "0.977\n",
      "0.5 0.24999999999999997 100\n",
      "0.977\n",
      "0.5 0.09999999999999999 100\n",
      "0.977\n",
      "0.1 0.5499999999999999 100\n",
      "0.977\n",
      "0.05 0.39999999999999997 100\n",
      "0.977\n",
      "1e-05 0.6 100\n",
      "0.977\n",
      "1e-05 0.6 180\n",
      "0.977\n",
      "0.1 0.24999999999999997 180\n",
      "0.977\n",
      "0.1 0.24999999999999997 140\n",
      "0.977\n",
      "0.1 0.24999999999999997 100\n",
      "0.977\n",
      "0.1 0.24999999999999997 40\n",
      "0.977\n",
      "0.5 0.0 140\n",
      "0.977\n",
      "1e-05 0.24999999999999997 140\n",
      "0.977\n",
      "0.1 0.44999999999999996 140\n",
      "0.977\n",
      "1e-05 0.24999999999999997 180\n",
      "0.977\n",
      "0.1 0.44999999999999996 180\n",
      "0.977\n",
      "1e-05 0.24999999999999997 100\n",
      "0.977\n",
      "0.1 0.44999999999999996 100\n",
      "0.977\n",
      "0.5 0.0 100\n",
      "0.977\n",
      "1e-05 0.24999999999999997 40\n",
      "0.977\n",
      "0.1 0.44999999999999996 40\n",
      "0.977\n",
      "0.5 0.0 180\n",
      "0.977\n",
      "1e-05 0.24999999999999997 120\n",
      "0.977\n",
      "1636.6296\n",
      "1e-05 0.0 40\n",
      "0.972\n",
      "1e-05 0.0 60\n",
      "0.972\n",
      "1e-05 0.0 80\n",
      "0.972\n",
      "1e-05 0.0 100\n",
      "0.972\n",
      "1e-05 0.0 120\n",
      "0.972\n",
      "1e-05 0.0 140\n",
      "0.972\n",
      "1e-05 0.0 160\n",
      "0.972\n",
      "1e-05 0.0 180\n",
      "0.972\n",
      "1e-05 0.0 200\n",
      "0.972\n",
      "0.5 0.35 40\n",
      "0.972\n",
      "0.5 0.39999999999999997 60\n",
      "0.972\n",
      "0.5 0.39999999999999997 80\n",
      "0.972\n",
      "0.5 0.39999999999999997 100\n",
      "0.974\n",
      "0.5 0.35 120\n",
      "0.974\n",
      "0.5 0.39999999999999997 140\n",
      "0.974\n",
      "0.5 0.39999999999999997 160\n",
      "0.974\n",
      "0.5 0.39999999999999997 180\n",
      "0.974\n",
      "0.5 0.39999999999999997 200\n",
      "0.975\n",
      "1e-05 0.6 40\n",
      "0.975\n",
      "1e-05 0.6 120\n",
      "0.975\n",
      "1e-05 0.6 60\n",
      "0.975\n",
      "1e-05 0.6 80\n",
      "0.975\n",
      "1e-05 0.6 100\n",
      "0.975\n",
      "1e-05 0.6 140\n",
      "0.975\n",
      "1e-05 0.6 160\n",
      "0.975\n",
      "1e-05 0.6 180\n",
      "0.975\n",
      "1e-05 0.6 200\n",
      "0.975\n",
      "0.5 0.0 60\n",
      "0.975\n",
      "0.5 0.0 80\n",
      "0.975\n",
      "0.5 0.0 100\n",
      "0.975\n",
      "0.5 0.0 140\n",
      "0.975\n",
      "0.5 0.0 160\n",
      "0.975\n",
      "0.5 0.0 180\n",
      "0.975\n",
      "0.5 0.0 200\n",
      "0.975\n",
      "0.5 0.0 40\n",
      "0.975\n",
      "0.5 0.0 120\n",
      "0.975\n",
      "0.1 0.3 60\n",
      "0.975\n",
      "0.1 0.3 80\n",
      "0.975\n",
      "0.1 0.3 100\n",
      "0.975\n",
      "0.1 0.3 140\n",
      "0.975\n",
      "0.1 0.3 160\n",
      "0.975\n",
      "0.1 0.3 180\n",
      "0.975\n",
      "0.1 0.3 200\n",
      "0.975\n",
      "0.1 0.3 40\n",
      "0.975\n",
      "0.1 0.3 120\n",
      "0.975\n",
      "0.5 0.6 40\n",
      "0.975\n",
      "0.5 0.6 120\n",
      "0.975\n",
      "0.5 0.6 60\n",
      "0.975\n",
      "0.5 0.6 80\n",
      "0.975\n",
      "0.5 0.6 100\n",
      "0.975\n",
      "0.5 0.6 140\n",
      "0.975\n",
      "0.5 0.6 160\n",
      "0.975\n",
      "0.5 0.6 180\n",
      "0.975\n",
      "0.5 0.6 200\n",
      "0.975\n",
      "0.5 0.19999999999999998 60\n",
      "0.975\n",
      "0.5 0.19999999999999998 80\n",
      "0.975\n",
      "0.5 0.19999999999999998 100\n",
      "0.975\n",
      "0.5 0.19999999999999998 140\n",
      "0.975\n",
      "0.5 0.19999999999999998 160\n",
      "0.975\n",
      "0.5 0.19999999999999998 180\n",
      "0.975\n",
      "0.5 0.19999999999999998 200\n",
      "0.975\n",
      "0.5 0.15 40\n",
      "0.975\n",
      "0.5 0.15 120\n",
      "0.975\n",
      "1e-05 0.15 60\n",
      "0.975\n",
      "1e-05 0.44999999999999996 80\n",
      "0.975\n",
      "1e-05 0.15 100\n",
      "0.975\n",
      "1e-05 0.15 140\n",
      "0.975\n",
      "1e-05 0.15 160\n",
      "0.975\n",
      "1e-05 0.15 180\n",
      "0.975\n",
      "1e-05 0.15 200\n",
      "0.975\n",
      "1e-05 0.44999999999999996 40\n",
      "0.975\n",
      "1e-05 0.44999999999999996 120\n",
      "0.975\n",
      "1e-05 0.15 80\n",
      "0.975\n",
      "1e-05 0.44999999999999996 60\n",
      "0.975\n",
      "1e-05 0.44999999999999996 100\n",
      "0.975\n",
      "1e-05 0.44999999999999996 140\n",
      "0.975\n",
      "1e-05 0.44999999999999996 160\n",
      "0.975\n",
      "1e-05 0.44999999999999996 180\n",
      "0.975\n",
      "1e-05 0.44999999999999996 200\n",
      "0.975\n",
      "1e-05 0.15 40\n",
      "0.975\n",
      "1e-05 0.15 120\n",
      "0.975\n",
      "0.1 0.5499999999999999 40\n",
      "0.975\n",
      "0.1 0.5499999999999999 120\n",
      "0.975\n",
      "0.1 0.049999999999999996 60\n",
      "0.975\n",
      "0.1 0.049999999999999996 80\n",
      "0.975\n",
      "0.1 0.049999999999999996 100\n",
      "0.975\n",
      "0.1 0.049999999999999996 140\n",
      "0.975\n",
      "0.1 0.049999999999999996 160\n",
      "0.975\n",
      "0.1 0.049999999999999996 180\n",
      "0.975\n",
      "0.1 0.049999999999999996 200\n",
      "0.975\n",
      "0.1 0.5499999999999999 60\n",
      "0.975\n",
      "0.1 0.5499999999999999 80\n",
      "0.975\n",
      "0.1 0.5499999999999999 100\n",
      "0.975\n",
      "0.1 0.5499999999999999 140\n",
      "0.975\n",
      "0.1 0.5499999999999999 160\n",
      "0.975\n",
      "0.1 0.5499999999999999 180\n",
      "0.975\n",
      "0.1 0.5499999999999999 200\n",
      "0.975\n",
      "0.1 0.049999999999999996 40\n",
      "0.975\n",
      "0.1 0.049999999999999996 120\n",
      "0.975\n",
      "0.5 0.49999999999999994 40\n",
      "0.975\n",
      "1585.2778\n",
      "1e-05 0.0 40\n",
      "0.972\n",
      "1e-05 0.0 60\n",
      "0.972\n",
      "1e-05 0.0 80\n",
      "0.972\n",
      "1e-05 0.0 100\n",
      "0.972\n",
      "1e-05 0.0 120\n",
      "0.972\n",
      "1e-05 0.0 140\n",
      "0.972\n",
      "1e-05 0.0 160\n",
      "0.972\n",
      "1e-05 0.0 180\n",
      "0.972\n",
      "1e-05 0.0 200\n",
      "0.972\n",
      "0.5 0.6 120\n",
      "0.972\n",
      "0.5 0.5499999999999999 120\n",
      "0.972\n",
      "0.5 0.44999999999999996 120\n",
      "0.972\n",
      "0.5 0.35 120\n",
      "0.972\n",
      "0.5 0.39999999999999997 120\n",
      "0.972\n",
      "0.5 0.24999999999999997 120\n",
      "0.972\n",
      "0.5 0.15 120\n",
      "0.972\n",
      "0.5 0.049999999999999996 120\n",
      "0.972\n",
      "0.5 0.09999999999999999 120\n",
      "0.973\n",
      "0.1 0.6 120\n",
      "0.973\n",
      "0.1 0.5499999999999999 120\n",
      "0.973\n",
      "0.05 0.5499999999999999 120\n",
      "0.973\n",
      "0.05 0.44999999999999996 120\n",
      "0.973\n",
      "0.1 0.39999999999999997 120\n",
      "0.973\n",
      "0.05 0.35 120\n",
      "0.973\n",
      "0.1 0.3 120\n",
      "0.973\n",
      "0.05 0.24999999999999997 120\n",
      "0.973\n",
      "0.1 0.19999999999999998 120\n",
      "0.973\n",
      "0.5 0.6 160\n",
      "0.973\n",
      "0.5 0.5499999999999999 160\n",
      "0.974\n",
      "0.5 0.44999999999999996 160\n",
      "0.974\n",
      "0.5 0.35 160\n",
      "0.974\n",
      "0.5 0.24999999999999997 160\n",
      "0.974\n",
      "0.5 0.15 160\n",
      "0.974\n",
      "0.5 0.049999999999999996 160\n",
      "0.974\n",
      "0.5 0.0 160\n",
      "0.974\n",
      "0.1 0.5499999999999999 160\n",
      "0.974\n",
      "0.05 0.6 160\n",
      "0.974\n",
      "0.05 0.49999999999999994 160\n",
      "0.974\n",
      "0.1 0.44999999999999996 160\n",
      "0.974\n",
      "0.05 0.35 160\n",
      "0.974\n",
      "0.1 0.3 160\n",
      "0.974\n",
      "0.1 0.35 160\n",
      "0.974\n",
      "1e-05 0.24999999999999997 160\n",
      "0.974\n",
      "0.05 0.39999999999999997 160\n",
      "0.974\n",
      "0.5 0.6 180\n",
      "0.974\n",
      "0.5 0.5499999999999999 180\n",
      "0.974\n",
      "0.5 0.39999999999999997 180\n",
      "0.974\n",
      "0.5 0.3 180\n",
      "0.974\n",
      "0.5 0.19999999999999998 180\n",
      "0.974\n",
      "0.5 0.09999999999999999 180\n",
      "0.974\n",
      "0.5 0.0 180\n",
      "0.974\n",
      "0.1 0.6 180\n",
      "0.974\n",
      "0.05 0.5499999999999999 180\n",
      "0.974\n",
      "0.01 0.6 180\n",
      "0.974\n",
      "0.1 0.44999999999999996 180\n",
      "0.974\n",
      "0.01 0.39999999999999997 180\n",
      "0.974\n",
      "0.1 0.35 180\n",
      "0.974\n",
      "0.1 0.24999999999999997 180\n",
      "0.974\n",
      "0.05 0.3 180\n",
      "0.974\n",
      "0.5 0.6 100\n",
      "0.974\n",
      "0.5 0.49999999999999994 100\n",
      "0.974\n",
      "0.5 0.39999999999999997 100\n",
      "0.974\n",
      "0.5 0.3 100\n",
      "0.974\n",
      "0.5 0.19999999999999998 100\n",
      "0.974\n",
      "0.5 0.09999999999999999 100\n",
      "0.974\n",
      "0.5 0.0 100\n",
      "0.974\n",
      "0.1 0.5499999999999999 100\n",
      "0.974\n",
      "0.01 0.5499999999999999 100\n",
      "0.974\n",
      "0.1 0.44999999999999996 100\n",
      "0.974\n",
      "0.01 0.35 100\n",
      "0.974\n",
      "0.1 0.3 100\n",
      "0.974\n",
      "0.5 0.6 40\n",
      "0.974\n",
      "0.5 0.49999999999999994 40\n",
      "0.974\n",
      "0.5 0.35 40\n",
      "0.974\n",
      "0.5 0.24999999999999997 40\n",
      "0.974\n",
      "0.5 0.09999999999999999 40\n",
      "0.974\n",
      "0.5 0.0 40\n",
      "0.974\n",
      "0.1 0.5499999999999999 40\n",
      "0.974\n",
      "1e-05 0.6 40\n",
      "0.974\n",
      "0.1 0.49999999999999994 40\n",
      "0.974\n",
      "0.5 0.6 80\n",
      "0.974\n",
      "0.5 0.49999999999999994 80\n",
      "0.974\n",
      "0.5 0.35 80\n",
      "0.974\n",
      "0.5 0.24999999999999997 80\n",
      "0.974\n",
      "0.5 0.15 80\n",
      "0.974\n",
      "0.5 0.049999999999999996 80\n",
      "0.974\n",
      "0.1 0.5499999999999999 80\n",
      "0.974\n",
      "0.005 0.5499999999999999 80\n",
      "0.974\n",
      "0.1 0.44999999999999996 80\n",
      "0.974\n",
      "0.01 0.35 80\n",
      "0.974\n",
      "0.5 0.6 200\n",
      "0.974\n",
      "0.5 0.49999999999999994 200\n",
      "0.974\n",
      "0.5 0.39999999999999997 200\n",
      "0.974\n",
      "0.5 0.3 200\n",
      "0.974\n",
      "0.5 0.19999999999999998 200\n",
      "0.974\n",
      "0.5 0.049999999999999996 200\n",
      "0.974\n",
      "0.1 0.5499999999999999 200\n",
      "0.974\n",
      "0.005 0.5499999999999999 200\n",
      "0.974\n",
      "0.1 0.44999999999999996 200\n",
      "0.974\n",
      "0.01 0.35 200\n",
      "0.974\n",
      "1590.1756\n"
     ]
    }
   ],
   "source": [
    "# now run the bayesian optimisation function with 3 different aquisition functions \n",
    "\n",
    "inputX2_UCB, outputY2_UCB = optimising_loop(inputX, outputY,100, aqfn = \"UCB\")\n",
    "inputX2_MV, outputY2_MV = optimising_loop(inputX, outputY,100, aqfn = \"max_var\")\n",
    "inputX2_POI, outputY2_POI = optimising_loop(inputX, outputY,100, aqfn = \"POI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the maximum value reached each run for plotting\n",
    "outY2max_UCB  = np.zeros(100)\n",
    "outY2max_MV  = np.zeros(100)\n",
    "outY2max_POI  = np.zeros(100)\n",
    "for ii in range(100):\n",
    "    outY2max_UCB[ii] = max(outputY2_UCB[:ii+1])\n",
    "    outY2max_MV[ii] = max(outputY2_MV[:ii+1])\n",
    "    outY2max_POI[ii] = max(outputY2_POI[:ii+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.145, 0.969, 0.939, 0.879, 0.731, 0.966, 0.096, 0.972, 0.76 ,\n",
       "       0.733, 0.113, 0.182, 0.153, 0.079, 0.122, 0.098, 0.157, 0.081,\n",
       "       0.111, 0.96 , 0.957, 0.959, 0.966, 0.97 , 0.943, 0.893, 0.095,\n",
       "       0.954, 0.093, 0.948, 0.963, 0.965, 0.968, 0.967, 0.974, 0.963,\n",
       "       0.875, 0.081, 0.958, 0.086, 0.957, 0.962, 0.961, 0.967, 0.965,\n",
       "       0.949, 0.081, 0.96 , 0.962, 0.976, 0.968, 0.971, 0.96 , 0.954,\n",
       "       0.128, 0.96 , 0.94 , 0.948, 0.938, 0.953, 0.961, 0.941, 0.923,\n",
       "       0.112, 0.968, 0.97 , 0.97 , 0.969, 0.969, 0.96 , 0.961, 0.083,\n",
       "       0.96 , 0.957, 0.965, 0.963, 0.977, 0.961, 0.939, 0.079, 0.962,\n",
       "       0.961, 0.971, 0.966, 0.96 , 0.943, 0.965, 0.963, 0.969, 0.971,\n",
       "       0.962, 0.937, 0.126, 0.15 , 0.955, 0.959, 0.957, 0.95 , 0.973,\n",
       "       0.089, 0.959, 0.176, 0.96 , 0.125, 0.956, 0.965, 0.117, 0.942,\n",
       "       0.97 , 0.095])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputY2_UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.145, 0.969, 0.969, 0.969, 0.969, 0.969, 0.969, 0.972, 0.972,\n",
       "       0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.972,\n",
       "       0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.972,\n",
       "       0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.972, 0.974, 0.974,\n",
       "       0.974, 0.974, 0.974, 0.974, 0.974, 0.974, 0.974, 0.974, 0.974,\n",
       "       0.974, 0.974, 0.974, 0.974, 0.976, 0.976, 0.976, 0.976, 0.976,\n",
       "       0.976, 0.976, 0.976, 0.976, 0.976, 0.976, 0.976, 0.976, 0.976,\n",
       "       0.976, 0.976, 0.976, 0.976, 0.976, 0.976, 0.976, 0.976, 0.976,\n",
       "       0.976, 0.976, 0.976, 0.976, 0.977, 0.977, 0.977, 0.977, 0.977,\n",
       "       0.977, 0.977, 0.977, 0.977, 0.977, 0.977, 0.977, 0.977, 0.977,\n",
       "       0.977, 0.977, 0.977, 0.977, 0.977, 0.977, 0.977, 0.977, 0.977,\n",
       "       0.977])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outY2max_UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now plot the results\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(1,101), outY2max_UCB, color = \"red\", label = \"UCB\")\n",
    "ax.plot(range(1,101), outY2max_MV, color = \"blue\", label = \"MV\")\n",
    "ax.plot(range(1,101), outY2max_POI, color = \"green\", label = \"POI\")\n",
    "ax.set_ylim(0.9,1.0)\n",
    "ax.legend(loc = \"best\")\n",
    "plt.savefig(\"chart1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the bayesian optimisation did help to find the optimum hyperparamters.  In this case the maximum variance acquisition function was the quickest to improve the hyperparameter values, but the UCB found the best result in the long run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
